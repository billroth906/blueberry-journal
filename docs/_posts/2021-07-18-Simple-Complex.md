---
layout: post
title: "Making the Simple Complex and then Trying to Make it Simple Again"
date: 2021-07-18 12:00:00 -0400
categories: sdc
---
### CSV to JSON

After being given the massive CSV files to populate my database, I turned to the MongoDB documentation to see what I needed for importing. While MongoDB will indeed import a CSV, the documentation also allows for a JSON import. Given that the ultimate document format and structure in MongoDB will be JSON's cousin BSON, it made the most sense to me to get the data into a shape that would be most useful in the end.
The first challenge then was figuring out how to convert from CSV to JSON. I tried earnestly to see if there was any standard, best practice to accomplish this, but never did stumble across one. I spent a while trying to implement a popular NPM package named PapaParse <https://www.papaparse.com/> and its sibling react-papaparse <https://react-papaparse.js.org/>, but in the end after I got that implementation running it seemed the resultant files were just too huge after PapaParse threw error messages and other information (or the lack thereof) onto each line it processed. I recognized that I needed a simpler solution, and I found it in the NPM csvtojson package. <https://www.npmjs.com/package/csvtojson> That package does one thing and it does it well, goes line by line, adds brackets, adds keys based on table column, and boom its done. But then to get that array in a useful format was another hurdle. The resultant arrays were too large for the native JSON.stringify. I eventually worked a solution where I was using Node's createWriteStream and JSON.stringifying every line as it was being written into the write stream. That process seemed to be working well. But then I started to think of the steps that I would need to combine to JSONs into a usiable file. That would require me to use readstreams to access the JSON files.
There is a seeming widely celebrated NPM package called JSONStream, that works with writestreams and readstreams in Node to stringify and parse the files, respectively. It works, and I've had success with it. I refactored the JSON.stringifying every line part to be handled by this package. But golly, the processing is extremely heavy on CPU and RAM. It has taken days on a few 48GB Ram 16CPU machines to get it done. And I'm still waiting on SKUs.

### Javascript for the Transformation
While waiting for the CSVs to process, I turned to plain old Javascript to write the process for combining all the huge JSONs into one. With a bunch of callbacks and a lot of FOR loops, I believe that I've gotten workable code written to make the master JSON for importing into MongoDB. But time is not on my side. Though my code appears functional, and isn't throwing any error messages, I know that when I actually go to run it (when all the JSONs are done), that it will take absolutely forever. I am happy to have slogged through this process and gotten to where I have the whole system for transforming these massive files. But I've run out of time.

### End of the Week Change
After working so hard to get the ETL process done in this Phase, I wasn't willing to have egg on my face and not meet BMR by the end of the weekend. So I did a quick about face. Decided that I would change my primary DBMS to PostgreSQL. The irony, of course, was that I have swimmingly been able to pull off the ETL process for the SQL database at the last minute (and with no major hurdles), but the NoSQL database (that which is advertised as being the one SO EASY to start with) was the process giving me fits. So PostgreSQL it is, for now. MongoDB is still, oh so close!